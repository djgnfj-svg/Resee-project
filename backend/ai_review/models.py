"""
AI Review models for generating and evaluating interactive questions
"""
from django.db import models
from django.contrib.auth import get_user_model
from django.core.validators import MinValueValidator, MaxValueValidator
from django.core.exceptions import ValidationError
from content.models import Content
from review.models import ReviewHistory


User = get_user_model()


class AIQuestionType(models.Model):
    """
    Types of AI-generated questions
    """
    QUESTION_TYPES = [
        ('multiple_choice', 'Multiple Choice'),
        ('fill_blank', 'Fill in the Blank'),
        ('blur_processing', 'Blur Processing'),
    ]
    
    name = models.CharField(
        max_length=50,
        choices=QUESTION_TYPES,
        unique=True,
        help_text="Internal name for the question type"
    )
    display_name = models.CharField(
        max_length=100,
        help_text="Human-readable name for the question type"
    )
    description = models.TextField(
        blank=True,
        help_text="Description of how this question type works"
    )
    is_active = models.BooleanField(
        default=True,
        help_text="Whether this question type is available for generation"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'ai_question_types'
        ordering = ['name']
    
    def __str__(self):
        return self.display_name


class AIQuestion(models.Model):
    """
    AI-generated questions for content review
    """
    content = models.ForeignKey(
        Content,
        on_delete=models.CASCADE,
        related_name='ai_questions',
        help_text="Content this question is based on"
    )
    question_type = models.ForeignKey(
        AIQuestionType,
        on_delete=models.CASCADE,
        related_name='questions',
        help_text="Type of question (multiple choice, short answer, etc.)"
    )
    question_text = models.TextField(
        help_text="The actual question text generated by AI"
    )
    correct_answer = models.TextField(
        help_text="The correct answer or expected response"
    )
    options = models.JSONField(
        null=True,
        blank=True,
        help_text="Multiple choice options (array of strings) or other structured data"
    )
    difficulty = models.IntegerField(
        validators=[MinValueValidator(1), MaxValueValidator(5)],
        default=1,
        help_text="Difficulty level from 1 (easy) to 5 (very hard)"
    )
    explanation = models.TextField(
        blank=True,
        help_text="AI-generated explanation of the correct answer"
    )
    keywords = models.JSONField(
        null=True,
        blank=True,
        help_text="Key concepts or terms this question covers"
    )
    ai_model_used = models.CharField(
        max_length=100,
        blank=True,
        help_text="AI model used to generate this question"
    )
    generation_prompt = models.TextField(
        blank=True,
        help_text="Prompt used to generate this question (for debugging)"
    )
    is_active = models.BooleanField(
        default=True,
        help_text="Whether this question is available for review sessions"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'ai_questions'
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['content', 'is_active']),
            models.Index(fields=['question_type', 'difficulty']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.content.title} - {self.question_type.display_name}"
    
    def clean(self):
        """
        Validate the question based on its type
        """
        super().clean()
        
        # Validate multiple choice options
        if self.question_type.name == 'multiple_choice':
            if not self.options or not isinstance(self.options, list):
                raise ValidationError("Multiple choice questions must have options as a list")
            if len(self.options) < 2:
                raise ValidationError("Multiple choice questions must have at least 2 options")
            if self.correct_answer not in self.options:
                raise ValidationError("Correct answer must be one of the options")



class AIEvaluation(models.Model):
    """
    AI evaluation of user answers to questions
    """
    question = models.ForeignKey(
        AIQuestion,
        on_delete=models.CASCADE,
        related_name='evaluations',
        help_text="Question that was answered"
    )
    user = models.ForeignKey(
        User,
        on_delete=models.CASCADE,
        related_name='ai_evaluations',
        help_text="User who answered the question"
    )
    user_answer = models.TextField(
        help_text="The user's answer to the question"
    )
    ai_score = models.FloatField(
        validators=[MinValueValidator(0.0), MaxValueValidator(1.0)],
        help_text="AI-generated score from 0.0 (incorrect) to 1.0 (perfect)"
    )
    feedback = models.TextField(
        blank=True,
        help_text="AI-generated feedback on the answer"
    )
    similarity_score = models.FloatField(
        null=True,
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(1.0)],
        help_text="Semantic similarity score between user answer and correct answer"
    )
    evaluation_details = models.JSONField(
        null=True,
        blank=True,
        help_text="Detailed evaluation breakdown (reasoning, partial scores, etc.)"
    )
    ai_model_used = models.CharField(
        max_length=100,
        blank=True,
        help_text="AI model used for evaluation"
    )
    processing_time_ms = models.IntegerField(
        null=True,
        blank=True,
        help_text="Time taken for AI evaluation in milliseconds"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'ai_evaluations'
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['user', 'created_at']),
            models.Index(fields=['question', 'ai_score']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        question_preview = self.question.question_text[:30] + "..." if len(self.question.question_text) > 30 else self.question.question_text
        return f"{self.user.email} - {question_preview} - {self.ai_score}"


class AIReviewSession(models.Model):
    """
    AI-enhanced review session tracking
    """
    review_history = models.OneToOneField(
        ReviewHistory,
        on_delete=models.CASCADE,
        related_name='ai_session',
        help_text="Associated traditional review history record"
    )
    questions_generated = models.IntegerField(
        default=0,
        validators=[MinValueValidator(0)],
        help_text="Number of AI questions generated for this session"
    )
    questions_answered = models.IntegerField(
        default=0,
        validators=[MinValueValidator(0)],
        help_text="Number of questions the user answered"
    )
    average_score = models.FloatField(
        null=True,
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(1.0)],
        help_text="Average AI score across all answered questions"
    )
    session_duration_seconds = models.IntegerField(
        null=True,
        blank=True,
        validators=[MinValueValidator(0)],
        help_text="Total time spent in the review session"
    )
    ai_processing_time_ms = models.IntegerField(
        null=True,
        blank=True,
        validators=[MinValueValidator(0)],
        help_text="Total time spent on AI processing during the session"
    )
    session_type = models.CharField(
        max_length=50,
        default='mixed',
        choices=[
            ('traditional', 'Traditional Review'),
            ('ai_only', 'AI Questions Only'),
            ('mixed', 'Mixed Traditional + AI'),
        ],
        help_text="Type of review session"
    )
    notes = models.TextField(
        blank=True,
        help_text="Session notes or observations"
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'ai_review_sessions'
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['review_history']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        user = self.review_history.user
        content = self.review_history.content
        return f"{user.email} - {content.title} - AI Session"
    
    @property
    def completion_percentage(self):
        """
        Calculate percentage of questions answered
        """
        if self.questions_generated == 0:
            return 0.0
        return (self.questions_answered / self.questions_generated) * 100.0
