"""
AI Answer Evaluation Views
"""
import logging

from django.shortcuts import get_object_or_404
from drf_yasg import openapi
from drf_yasg.utils import swagger_auto_schema
from rest_framework import status
from rest_framework.response import Response

from ..models import AIQuestion
from ..serializers import (
    ExplanationEvaluationRequestSerializer,
    ExplanationEvaluationResponseSerializer
)
from ..services import AIServiceError
from .base import BaseAIView

logger = logging.getLogger(__name__)


class AIAnswerEvaluationView(BaseAIView):
    """
    Evaluate user answers using AI
    """
    
    @swagger_auto_schema(
        operation_summary="AI ë‹µë³€ í‰ê°€",
        operation_description="""
        ì‚¬ìš©ìì˜ ë‹µë³€ì„ AIë¡œ í‰ê°€í•˜ì—¬ ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
        
        **ìš”ì²­ ì˜ˆì‹œ:**
        ```json
        {
          "question_id": 123,
          "user_answer": "Pythonì€ í•´ì„í˜• ì–¸ì–´ì…ë‹ˆë‹¤."
        }
        ```
        
        **ì‘ë‹µ ì˜ˆì‹œ:**
        ```json
        {
          "id": 456,
          "score": 0.85,
          "feedback": "ì •ë‹µì— ê°€ê¹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°ì²´ì§€í–¥ íŠ¹ì„±ë„ ì–¸ê¸‰í•˜ë©´ ë” ì™„ì „í•œ ë‹µë³€ì´ ë©ë‹ˆë‹¤.",
          "similarity_score": 0.78,
          "evaluation_details": {
            "strengths": ["ê¸°ë³¸ ê°œë… ì´í•´ê°€ ì •í™•í•¨"],
            "weaknesses": ["ë¶€ë¶„ì  ì„¤ëª…"],
            "suggestions": ["ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¼ëŠ” ì ë„ ì¶”ê°€í•´ë³´ì„¸ìš”"]
          }
        }
        ```
        
        **ì±„ì  ê¸°ì¤€:**
        - 0.9-1.0: ìš°ìˆ˜ (ì™„ë²½í•œ ì´í•´)
        - 0.7-0.8: ì–‘í˜¸ (ëŒ€ë¶€ë¶„ ì •í™•)
        - 0.5-0.6: ë³´í†µ (ë¶€ë¶„ì  ì´í•´)
        - 0.3-0.4: ë¯¸í¡ (ê¸°ë³¸ ì´í•´ ë¶€ì¡±)
        - 0.0-0.2: ë¶€ì¡± (ì´í•´ ë¶€ì¡±)
        """,
        tags=['AI Review'],
        request_body=openapi.Schema(
            type=openapi.TYPE_OBJECT,
            properties={
                'question_id': openapi.Schema(type=openapi.TYPE_INTEGER, description="í‰ê°€í•  ì§ˆë¬¸ ID"),
                'user_answer': openapi.Schema(type=openapi.TYPE_STRING, description="ì‚¬ìš©ìì˜ ë‹µë³€"),
            },
            required=['question_id', 'user_answer']
        ),
        responses={
            201: openapi.Response(
                description="ë‹µë³€ í‰ê°€ ì™„ë£Œ",
                schema=openapi.Schema(
                    type=openapi.TYPE_OBJECT,
                    properties={
                        'id': openapi.Schema(type=openapi.TYPE_INTEGER, description="í‰ê°€ ê¸°ë¡ ID"),
                        'score': openapi.Schema(type=openapi.TYPE_NUMBER, description="AI í‰ê°€ ì ìˆ˜ (0.0-1.0)"),
                        'feedback': openapi.Schema(type=openapi.TYPE_STRING, description="AI í”¼ë“œë°±"),
                        'similarity_score': openapi.Schema(type=openapi.TYPE_NUMBER, description="ì˜ë¯¸ì  ìœ ì‚¬ë„ ì ìˆ˜"),
                        'evaluation_details': openapi.Schema(
                            type=openapi.TYPE_OBJECT,
                            properties={
                                'strengths': openapi.Schema(type=openapi.TYPE_ARRAY, items=openapi.Schema(type=openapi.TYPE_STRING)),
                                'weaknesses': openapi.Schema(type=openapi.TYPE_ARRAY, items=openapi.Schema(type=openapi.TYPE_STRING)),
                                'suggestions': openapi.Schema(type=openapi.TYPE_ARRAY, items=openapi.Schema(type=openapi.TYPE_STRING)),
                            }
                        ),
                        'processing_time_ms': openapi.Schema(type=openapi.TYPE_INTEGER, description="AI ì²˜ë¦¬ ì‹œê°„"),
                    }
                )
            ),
            400: 'ì˜ëª»ëœ ìš”ì²­ - ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨',
            403: 'AI ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€ - êµ¬ë… ì—…ê·¸ë ˆì´ë“œ í•„ìš”',
            404: 'ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ',
            429: 'ì¼ì¼ í•œë„ ì´ˆê³¼',
            503: 'AI ì„œë¹„ìŠ¤ ì¼ì‹œì  ì‚¬ìš© ë¶ˆê°€',
            500: 'AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜'
        }
    )
    def post(self, request):
        """Evaluate user answer with AI"""
        # AI ì„œë¹„ìŠ¤ ë¯¸êµ¬í˜„ ì•Œë¦¼
        return Response(
            {
                'error': 'AI ì„œë¹„ìŠ¤ ë¯¸êµ¬í˜„',
                'detail': 'AI ë‹µë³€ í‰ê°€ ê¸°ëŠ¥ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤. ê³§ ì œê³µë  ì˜ˆì •ì´ë‹ˆ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”! ğŸš€',
                'status': 'under_development'
            },
            status=status.HTTP_501_NOT_IMPLEMENTED
        )
        
        # Implementation plan:
        """
        Future implementation will include:
        1. AI feature access validation
        2. Answer evaluation using AI service
        3. Feedback generation and scoring
        4. Usage tracking and analytics
        
        Currently returns not implemented status.
        
        Example implementation structure:
        # Check AI feature access
        access_response = self.check_ai_feature_access(request)
        if access_response:
            return access_response
        
        question_id = request.data.get('question_id')
        user_answer = request.data.get('user_answer')
        
        if not question_id or not user_answer:
            return Response(
                {'error': 'question_id and user_answer are required'},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # Check daily usage limit
        limit_response = self.check_daily_limit(request, 1)  # 1 evaluation = 1 credit
        if limit_response:
            return limit_response
        
        # Get question and verify ownership through content
        try:
            question = AIQuestion.objects.select_related('content').get(
                id=question_id,
                content__author=request.user,
                is_active=True
            )
        except AIQuestion.DoesNotExist:
            return Response(
                {'error': 'Question not found or access denied'},
                status=status.HTTP_404_NOT_FOUND
            )
        
        try:
            # Evaluate answer using AI service
            evaluation_result = self.ai_service.evaluate_answer(question, user_answer)
            
            # Create evaluation record
            evaluation = AIEvaluation.objects.create(
                question=question,
                user=request.user,
                user_answer=user_answer,
                ai_score=evaluation_result['score'],
                feedback=evaluation_result.get('feedback', ''),
                similarity_score=evaluation_result.get('similarity_score'),
                evaluation_details=evaluation_result.get('evaluation_details'),
                ai_model_used=evaluation_result.get('ai_model_used', ''),
                processing_time_ms=evaluation_result.get('processing_time_ms')
            )
            
            # Track usage
            self.track_usage(request, 1)
            
            logger.info(
                f"AI answer evaluation completed for question {question.id} "
                f"(user: {request.user.email}, score: {evaluation.ai_score})"
            )
            
            # Return evaluation result
            response_data = {
                'id': evaluation.id,
                'score': evaluation.ai_score,
                'feedback': evaluation.feedback,
                'similarity_score': evaluation.similarity_score,
                'evaluation_details': evaluation.evaluation_details,
                'processing_time_ms': evaluation.processing_time_ms,
                'question_id': question.id,
                'user_answer': user_answer
            }
            
            return Response(response_data, status=status.HTTP_201_CREATED)
            
        except AIServiceError as e:
            return self.handle_ai_service_error(e, request.user.email)
        except Exception as e:
            return self.handle_unexpected_error(e, 'Answer evaluation')
        """


class ExplanationEvaluationView(BaseAIView):
    """
    Evaluate user's descriptive explanation using AI
    """
    
    @swagger_auto_schema(
        operation_summary="ì„œìˆ í˜• ì„¤ëª… í‰ê°€",
        operation_description="""
        ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì„œìˆ í˜• ì„¤ëª…ì„ AIë¡œ í‰ê°€í•˜ì—¬ ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
        
        **ìš”ì²­ ì˜ˆì‹œ:**
        ```json
        {
          "content_id": 123,
          "user_explanation": "Pythonì€ í•´ì„í˜• ì–¸ì–´ë¡œì„œ ì½”ë“œë¥¼ í•œ ì¤„ì”© ì‹¤í–‰í•©ë‹ˆë‹¤..."
        }
        ```
        
        **ì‘ë‹µ ì˜ˆì‹œ:**
        ```json
        {
          "score": 85,
          "feedback": "í•µì‹¬ ê°œë…ì„ ì˜ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°ì²´ì§€í–¥ íŠ¹ì„±ë„ ì–¸ê¸‰í•˜ë©´ ë” ì™„ì „í•œ ì„¤ëª…ì´ ë©ë‹ˆë‹¤.",
          "strengths": ["ê¸°ë³¸ ê°œë… ì´í•´ê°€ ì •í™•í•¨", "ë…¼ë¦¬ì  êµ¬ì¡°ê°€ ì¢‹ìŒ"],
          "improvements": ["ë” êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì¶”ê°€", "ê°ì²´ì§€í–¥ íŠ¹ì„± ì–¸ê¸‰"],
          "key_concepts_covered": ["í•´ì„í˜• ì–¸ì–´", "ì‹¤í–‰ ë°©ì‹"],
          "missing_concepts": ["ê°ì²´ì§€í–¥", "ë™ì  íƒ€ì´í•‘"]
        }
        ```
        
        **í‰ê°€ ê¸°ì¤€:**
        - í•µì‹¬ ê°œë… ì´í•´ë„ (40%)
        - ì„¤ëª…ì˜ ì™„ì„±ë„ (30%)
        - ë…¼ë¦¬ì  êµ¬ì¡° (20%)
        - êµ¬ì²´ì  ì˜ˆì‹œë‚˜ ì„¸ë¶€ì‚¬í•­ (10%)
        
        **êµ¬ë… í‹°ì–´ ì œí•œ:**
        - Basic ì´ìƒ í‹°ì–´ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
        """,
        tags=['AI Review'],
        request_body=ExplanationEvaluationRequestSerializer,
        responses={
            200: ExplanationEvaluationResponseSerializer,
            400: 'ì˜ëª»ëœ ìš”ì²­ - ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨',
            403: 'êµ¬ë… í‹°ì–´ ë¶€ì¡± - Basic ì´ìƒ í•„ìš”',
            404: 'ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ',
            429: 'ì¼ì¼ í•œë„ ì´ˆê³¼',
            503: 'AI ì„œë¹„ìŠ¤ ì¼ì‹œì  ì‚¬ìš© ë¶ˆê°€',
            500: 'AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜'
        }
    )
    def post(self, request):
        """Evaluate user's descriptive explanation"""
        # Check AI feature access
        access_response = self.check_ai_feature_access(request, 'explanation_evaluation')
        if access_response:
            return access_response
        
        serializer = ExplanationEvaluationRequestSerializer(
            data=request.data,
            context={'request': request}
        )
        
        if not serializer.is_valid():
            return Response(
                serializer.errors,
                status=status.HTTP_400_BAD_REQUEST
            )
        
        content_id = serializer.validated_data['content_id']
        user_explanation = serializer.validated_data['user_explanation']
        
        # Check daily usage limit
        limit_response = self.check_daily_limit(request, 1)  # 1 evaluation = 1 credit
        if limit_response:
            return limit_response
        
        # Get content
        content = self.get_user_content(request, content_id)
        
        try:
            # Evaluate explanation using AI service
            result = self.ai_service.evaluate_explanation(
                content=content,
                user_explanation=user_explanation
            )
            
            # Track usage
            self.track_usage(request, 1)
            
            logger.info(
                f"AI explanation evaluation for content {content.id} "
                f"(user: {request.user.email}, score: {result['score']})"
            )
            
            return Response(result, status=status.HTTP_200_OK)
            
        except AIServiceError as e:
            return self.handle_ai_service_error(e, request.user.email)
        except Exception as e:
            return self.handle_unexpected_error(e, 'Explanation evaluation')